{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHlWT5IX_Tyr"
   },
   "source": [
    "## Steps included in this project\n",
    "- Loading the Dataset\n",
    "- Pre-processing the raw data\n",
    "- Getting BERT Pre-trained model and its tokenizer\n",
    "- Training and evaluation\n",
    "- Prediction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YIlTPyPW8isD",
    "outputId": "3ed2ea92-da16-40a6-808a-3b1516ac6ac9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OkRQ3yd9P7f0"
   },
   "source": [
    "# Loading the dataset\n",
    "The dataset we are using here is personality classification of german metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "id": "ZkUhXv0T_EBV",
    "outputId": "fcd54d8a-522d-4e80-ce8e-64af0293846e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-89440d33-f8db-43c8-a57f-71905d3996bb\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Geschlecht</th>\n",
       "      <th>Alter</th>\n",
       "      <th>Text</th>\n",
       "      <th>Extraversion_Classification</th>\n",
       "      <th>Gewissenhaftigkeit_Classification</th>\n",
       "      <th>EmotionaleStabilitaet_Classification</th>\n",
       "      <th>Offenheit_Classification</th>\n",
       "      <th>Empathie_Classification</th>\n",
       "      <th>Wirksamkeitsueberzeugung_Classification</th>\n",
       "      <th>Optimismus_Classification</th>\n",
       "      <th>Resilienz_Classification</th>\n",
       "      <th>UnternehmerischesKapital_Classification</th>\n",
       "      <th>AgilityMindset_Classification</th>\n",
       "      <th>Machiavellismus_Classification</th>\n",
       "      <th>Narzissmus_Classification</th>\n",
       "      <th>Psychopathie_Classification</th>\n",
       "      <th>ZerstoererischesPotential_Classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>M</td>\n",
       "      <td>1972</td>\n",
       "      <td>Ich habe meinen Vater bis zu seinem Tode gepfl...</td>\n",
       "      <td>LOW</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>LOW</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>F</td>\n",
       "      <td>1976</td>\n",
       "      <td>Ich habe die Aufnahmeprüfung beim Landesgymnas...</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24</td>\n",
       "      <td>F</td>\n",
       "      <td>1966</td>\n",
       "      <td>Bei meinem Sohn wurde in Kindertagen ADHS diag...</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>F</td>\n",
       "      <td>1966</td>\n",
       "      <td>Mein größter Erfolg war, dass es mir trotz mei...</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49</td>\n",
       "      <td>M</td>\n",
       "      <td>1975</td>\n",
       "      <td>Ich habe siebeneinhalb Wohnungen gekauft. Eine...</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>LOW</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>HIGH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-89440d33-f8db-43c8-a57f-71905d3996bb')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-89440d33-f8db-43c8-a57f-71905d3996bb button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-89440d33-f8db-43c8-a57f-71905d3996bb');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   id Geschlecht  Alter                                               Text  \\\n",
       "0  15          M   1972  Ich habe meinen Vater bis zu seinem Tode gepfl...   \n",
       "1  20          F   1976  Ich habe die Aufnahmeprüfung beim Landesgymnas...   \n",
       "2  24          F   1966  Bei meinem Sohn wurde in Kindertagen ADHS diag...   \n",
       "3  45          F   1966  Mein größter Erfolg war, dass es mir trotz mei...   \n",
       "4  49          M   1975  Ich habe siebeneinhalb Wohnungen gekauft. Eine...   \n",
       "\n",
       "  Extraversion_Classification Gewissenhaftigkeit_Classification  \\\n",
       "0                         LOW                           AVERAGE   \n",
       "1                     AVERAGE                               LOW   \n",
       "2                         LOW                              HIGH   \n",
       "3                     AVERAGE                               LOW   \n",
       "4                     AVERAGE                               LOW   \n",
       "\n",
       "  EmotionaleStabilitaet_Classification Offenheit_Classification  \\\n",
       "0                                 HIGH                  AVERAGE   \n",
       "1                              AVERAGE                     HIGH   \n",
       "2                              AVERAGE                     HIGH   \n",
       "3                                  LOW                      LOW   \n",
       "4                                  LOW                     HIGH   \n",
       "\n",
       "  Empathie_Classification Wirksamkeitsueberzeugung_Classification  \\\n",
       "0                     LOW                                 AVERAGE   \n",
       "1                 AVERAGE                                 AVERAGE   \n",
       "2                    HIGH                                 AVERAGE   \n",
       "3                 AVERAGE                                     LOW   \n",
       "4                     LOW                                 AVERAGE   \n",
       "\n",
       "  Optimismus_Classification Resilienz_Classification  \\\n",
       "0                       LOW                  AVERAGE   \n",
       "1                       LOW                  AVERAGE   \n",
       "2                      HIGH                  AVERAGE   \n",
       "3                   AVERAGE                      LOW   \n",
       "4                   AVERAGE                  AVERAGE   \n",
       "\n",
       "  UnternehmerischesKapital_Classification AgilityMindset_Classification  \\\n",
       "0                                 AVERAGE                       AVERAGE   \n",
       "1                                 AVERAGE                       AVERAGE   \n",
       "2                                    HIGH                       AVERAGE   \n",
       "3                                     LOW                           LOW   \n",
       "4                                 AVERAGE                       AVERAGE   \n",
       "\n",
       "  Machiavellismus_Classification Narzissmus_Classification  \\\n",
       "0                           HIGH                       LOW   \n",
       "1                        AVERAGE                   AVERAGE   \n",
       "2                            LOW                   AVERAGE   \n",
       "3                            LOW                      HIGH   \n",
       "4                        AVERAGE                      HIGH   \n",
       "\n",
       "  Psychopathie_Classification ZerstoererischesPotential_Classification  \n",
       "0                     AVERAGE                                  AVERAGE  \n",
       "1                     AVERAGE                                  AVERAGE  \n",
       "2                     AVERAGE                                  AVERAGE  \n",
       "3                     AVERAGE                                  AVERAGE  \n",
       "4                        HIGH                                     HIGH  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel('/content/drive/MyDrive/data/data.xlsx')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b6bvsxYXAZyx",
    "outputId": "68e09246-4c41-423d-ace3-fd4fe46c1195"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1980 entries, 0 to 1979\n",
      "Data columns (total 18 columns):\n",
      " #   Column                                    Non-Null Count  Dtype \n",
      "---  ------                                    --------------  ----- \n",
      " 0   id                                        1980 non-null   int64 \n",
      " 1   Geschlecht                                1980 non-null   object\n",
      " 2   Alter                                     1980 non-null   int64 \n",
      " 3   Text                                      1980 non-null   object\n",
      " 4   Extraversion_Classification               1980 non-null   object\n",
      " 5   Gewissenhaftigkeit_Classification         1980 non-null   object\n",
      " 6   EmotionaleStabilitaet_Classification      1980 non-null   object\n",
      " 7   Offenheit_Classification                  1980 non-null   object\n",
      " 8   Empathie_Classification                   1980 non-null   object\n",
      " 9   Wirksamkeitsueberzeugung_Classification   1980 non-null   object\n",
      " 10  Optimismus_Classification                 1980 non-null   object\n",
      " 11  Resilienz_Classification                  1980 non-null   object\n",
      " 12  UnternehmerischesKapital_Classification   1980 non-null   object\n",
      " 13  AgilityMindset_Classification             1980 non-null   object\n",
      " 14  Machiavellismus_Classification            1980 non-null   object\n",
      " 15  Narzissmus_Classification                 1980 non-null   object\n",
      " 16  Psychopathie_Classification               1980 non-null   object\n",
      " 17  ZerstoererischesPotential_Classification  1980 non-null   object\n",
      "dtypes: int64(2), object(16)\n",
      "memory usage: 278.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HUeNzo_TBYQw",
    "outputId": "ee2158ea-ae62-4d5f-e39d-08478bd28517"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AVERAGE    860\n",
       "LOW        650\n",
       "HIGH       470\n",
       "Name: Extraversion_Classification, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Extraversion_Classification'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FkhAn4AYQSu1"
   },
   "source": [
    "# Splitting the dataset\n",
    "We initially create the training dataset with a fraction of 0.8 from overall rows in Dataframe. we also define **random_state** which corresponds to the seed, so that the results are reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "643Ho4wQQNUt",
    "outputId": "d0fbc08d-75c0-4868-ae98-fc02641f12a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of training examples: 1584\n",
      "No. of testing examples: 396\n"
     ]
    }
   ],
   "source": [
    "#df_train = df.sample(frac=0.8, random_state=25)\n",
    "#df_test = df.drop(df_train.index)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=25)\n",
    "\n",
    "print(f\"No. of training examples: {df_train.shape[0]}\")\n",
    "print(f\"No. of testing examples: {df_test.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXnNBcr7BhpS"
   },
   "source": [
    "# Converting our **Extraversion_Classification** into Categorical data\n",
    "- Mapping sentiments label with some numbers using a python dictionary and then convert them into a categorical column using to_categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "0IPtKuQxBajJ"
   },
   "outputs": [],
   "source": [
    "encoded_dict = {'LOW':0, 'AVERAGE':1, 'HIGH':2}\n",
    "df_train['Extraversion_Classification']=df.Extraversion_Classification.map(encoded_dict)\n",
    "df_test['Extraversion_Classification']=df.Extraversion_Classification.map(encoded_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJ9Zbf5fCgyA"
   },
   "source": [
    "importing to_categorical class from utils:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "n06A0jSvCGts"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import  to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8xd9GiGyCsJk"
   },
   "source": [
    "converting our integer coded Sentiment column into categorical data(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "oRtKKvFRCma2"
   },
   "outputs": [],
   "source": [
    "y_train = to_categorical(df_train.Extraversion_Classification)\n",
    "y_test = to_categorical(df_test.Extraversion_Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0oDeh_-JC49W",
    "outputId": "30e7c314-ab0b-4a75-c195-ad307e258163"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        ...,\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.]], dtype=float32), array([[0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        ...,\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.]], dtype=float32))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train,y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZDJBZHOC_wb"
   },
   "source": [
    "We have successfully processed our Extraversion_classification column( target); now, it’s time to process our input text data using a tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAvQSi52DLQU"
   },
   "source": [
    "# Getting transformers package\n",
    "- installing transformer package and then import it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HtmLN6EWC6si"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3vELy38DhOp"
   },
   "source": [
    "Loading Model and Tokenizer from the transformers package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q47QDo5UDUb_"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,TFBertModel\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-german-cased')\n",
    "bert = TFBertModel.from_pretrained('bert-base-german-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZs6FPkhEsBs"
   },
   "source": [
    "We need a tokenizer to convert the input text's word into tokens:\n",
    "- The **classAutoTokenizer** contains various types of tokenizers.\n",
    "- **TFBertModel** pre-trained Bert model for TensorFlow\n",
    "- Here We are loading **bert-base-german-cased** model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qtZ9aHL5DzZj",
    "outputId": "6bf049af-8ff9-45fb-9571-1bb69f493399"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ich',\n",
       " 'liebe',\n",
       " 'Deutschland',\n",
       " '.',\n",
       " 'Es',\n",
       " 'ist',\n",
       " 'ein',\n",
       " 'schöne',\n",
       " '##s',\n",
       " 'Land']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"ich liebe Deutschland. Es ist ein schönes Land\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RuR0PDu_FvnB"
   },
   "source": [
    "# Input Data Modeling\n",
    "Before training, we need to convert the input textual data into BERT's inout data format using a tokenizer. Since we have loaded **bert-base-german-cased**, so tokenizer will also be **Bert-base-german-cased**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "odrNEml6FQML"
   },
   "outputs": [],
   "source": [
    "# Tokenize the input (takes some time) \n",
    "# here tokenizer using from bert-base-german-cased\n",
    "x_train = tokenizer(\n",
    "    text=df_train.Text.tolist(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=256,\n",
    "    truncation=True,\n",
    "    padding=True, \n",
    "    return_tensors='tf',\n",
    "    return_token_type_ids = False,\n",
    "    return_attention_mask = True,\n",
    "    verbose = True)\n",
    "x_test = tokenizer(\n",
    "    text=df_test.Text.tolist(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=256,\n",
    "    truncation=True,\n",
    "    padding=True, \n",
    "    return_tensors='tf',\n",
    "    return_token_type_ids = False,\n",
    "    return_attention_mask = True,\n",
    "    verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mKv2NwHGdv-"
   },
   "source": [
    "Tokenizer takes all the necessary parameters and returns tensor in the same format Bert accepts.\n",
    "\n",
    "- **return_token_type_ids = False**: token_type_ids is not necessary for our training in this case.\n",
    "- **return_attention_mask = True** we want to include attention_mask in our input.\n",
    "- **return_tensors=’tf’**: we want our input tensor for the TensorFlow model.\n",
    "- **max_length=256:**\n",
    "  - we want the maximum length of each sentence to be 70; if a sentence is bigger than this, it will be trimmed if a sentence is smaller than\n",
    "70 then it will be padded.\n",
    "- **add_special_tokens=True**, CLS, SEP token will be added in the tokenization.\n",
    "\n",
    "Hereafter data modelling, the tokenizer will return a dictionary (X) containing ‘Input_ids’, ‘attention_mask’ as key for their respective\n",
    "data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "OUSK8QWMGb2A"
   },
   "outputs": [],
   "source": [
    "input_ids = x_train['input_ids']\n",
    "attention_mask = x_train['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GjD28aXa0kDN",
    "outputId": "0f85b376-34bd-4b17-93d5-7af4c59900ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(396, 256), dtype=int32, numpy=\n",
       "array([[    3,  3599, 16621, ...,     0,     0,     0],\n",
       "       [    3,    62, 26914, ...,     0,     0,     0],\n",
       "       [    3,  3599, 16621, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [    3,  1671,   555, ...,     0,     0,     0],\n",
       "       [    3,    39,   391, ...,     0,     0,     0],\n",
       "       [    3, 15430,  9651, ...,     0,     0,     0]], dtype=int32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V_WwLME7Sw8d"
   },
   "source": [
    "# Model Building\n",
    "Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "N2PEi4vrSwIY"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "jZxE3-sVIfEq"
   },
   "outputs": [],
   "source": [
    "max_len = 256\n",
    "input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n",
    "input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n",
    "embeddings = bert(input_ids,attention_mask = input_mask)[0] \n",
    "out = tf.keras.layers.GlobalMaxPool1D()(embeddings)\n",
    "out = Dense(128, activation='relu')(out)\n",
    "out = tf.keras.layers.Dropout(0.1)(out)\n",
    "out = Dense(32,activation = 'relu')(out)\n",
    "y = Dense(3,activation = 'sigmoid')(out)\n",
    "sentiment_model = tf.keras.Model(inputs=[input_ids, input_mask], outputs=y)\n",
    "sentiment_model.layers[2].trainable = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2E4YC1ZTICQ"
   },
   "source": [
    "Bert layers accept three input arrays, **input_ids,attention_mask, toke_type_ids**\n",
    "**input_ids** meabs our input words encoding, then **attention mask**.\n",
    "**token_type_ids** is necessary for question answering model: in this case we will not pass token_type_ids.\n",
    "- For the Bert layer, we need two input layers, in this case, input_ids, attention_mask.\n",
    "- **Embeddings** contain hidden states of the Bert layer.\n",
    "using\n",
    "GlobalMaxPooling1D then dense layer to build CNN layers using hidden\n",
    "states of Bert. These CNN layers will yield our output.\n",
    "bert[0] is the last hidden state, bert[1] is the\n",
    "pooler_output, for building CNN layers on top of the BERT layer, we have\n",
    "used Bert’s hidden forms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hJKkpT2TsiI"
   },
   "source": [
    "# Model compilation\n",
    "Defining learning parameters and compiling the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "4CB0WIjqHVSy"
   },
   "outputs": [],
   "source": [
    "optimizer = Adam(\n",
    "    learning_rate=5e-05, # this learning rate is for bert model , taken from huggingface website \n",
    "    epsilon=1e-08,\n",
    "    decay=0.01,\n",
    "    clipnorm=1.0)\n",
    "# Set loss and metrics\n",
    "loss =CategoricalCrossentropy(from_logits = True)\n",
    "metric = CategoricalAccuracy('balanced_accuracy'),\n",
    "#metric = [tf.keras.metrics.BinaryAccuracy(name='accuracy'),tf.keras.metrics.Precision(name='precision'),tf.keras.metrics.Recall(name='recall')]\n",
    "# Compile the model\n",
    "sentiment_model.compile(optimizer = optimizer,loss = loss, metrics = metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VybkeAFWCVl6"
   },
   "source": [
    "**learning_rate = 5e-05** the learning rate for the model will be significantly lower.\n",
    "\n",
    "**Loss = CategoricalCrossentropy** since we are passing the categorical data as the target.\n",
    "\n",
    "**Balanced accuracy** will take care of our average accuracy for all the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jJmHT7zRkCJQ",
    "outputId": "336a40b2-3f8e-4ce0-d7de-dc1310139c69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109081344   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 256,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " global_max_pooling1d_1 (Global  (None, 768)         0           ['tf_bert_model[1][0]']          \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 128)          98432       ['global_max_pooling1d_1[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_38 (Dropout)           (None, 128)          0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 32)           4128        ['dropout_38[0][0]']             \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 3)            99          ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,184,003\n",
      "Trainable params: 109,184,003\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sentiment_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPrAE6ApUDR5"
   },
   "source": [
    "#Model training\n",
    "we have the model ready with X_train, y_train. we can train the model.\n",
    "Training and fine tuning of the BERT model takes time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FEposPPkgola",
    "outputId": "0ca9e03a-fe5f-4d88-d3ff-06b50ada29d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1584, 256)\n",
      "(1584, 256)\n",
      "(1584, 3)\n",
      "[0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "x ={'input_ids':x_train['input_ids'],'attention_mask':x_train['attention_mask']}\n",
    "print(x['input_ids'].shape)\n",
    "print(x['attention_mask'].shape)\n",
    "print(y_train.shape)\n",
    "print(y_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iW01MmQyT2Yd",
    "outputId": "8444be13-e3e2-482c-acba-92bbc9aab623"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 120s 2s/step - loss: 1.1605 - balanced_accuracy: 0.3699 - val_loss: 1.0851 - val_balanced_accuracy: 0.3763\n"
     ]
    }
   ],
   "source": [
    "train_history = sentiment_model.fit(\n",
    "    x ={'input_ids':x_train['input_ids'],'attention_mask':x_train['attention_mask']} ,\n",
    "    y = y_train,\n",
    "    validation_data = ({'input_ids':x_test['input_ids'],'attention_mask':x_test['attention_mask']}, y_test),epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E80Oml7HC2wh"
   },
   "source": [
    "- **model.fit** returns a history object which keeps all the training history.\n",
    "- **x_test** became a dictionary containing ‘input_ids’, ‘attention_mask‘ after pre-processing. We are passing input_ids and attention_mask for the training.\n",
    "- In the validation data, we are passing the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8PuoXXgMU1Ns"
   },
   "source": [
    "# Model Evaluation\n",
    "Testing our model on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "MpFDsrC2qhGs"
   },
   "outputs": [],
   "source": [
    "x_test = {'input_ids':x_test['input_ids'],'attention_mask':x_test['attention_mask']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ATAd0vuRn6DO",
    "outputId": "0a372954-68c1-46dc-b359-3e01f70ea933"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.59371257, 0.63438976, 0.56323344],\n",
       "       [0.5728156 , 0.58896255, 0.55890733],\n",
       "       [0.5942178 , 0.635093  , 0.60654104],\n",
       "       ...,\n",
       "       [0.588902  , 0.6130559 , 0.57000756],\n",
       "       [0.5423725 , 0.5897281 , 0.55537117],\n",
       "       [0.5250233 , 0.62670827, 0.5935005 ]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_raw = sentiment_model.predict(x_test)\n",
    "predicted_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N3DwT00YUDhq"
   },
   "source": [
    "Taking the index of value having maximum probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "1oF3doDGplRJ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y_predicted = np.argmax(predicted_raw,axis=1)\n",
    "y_true = np.array(df_test['Extraversion_Classification'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Vjk9IC7sLTb",
    "outputId": "2228d378-2b80-4812-ef1c-ee847a23982f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1,\n",
       "       2, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 2, 1, 2, 1, 1, 1, 1, 1,\n",
       "       2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 2, 0, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 2, 1, 1, 2, 1, 2, 2, 1,\n",
       "       2, 0, 1, 1, 1, 1, 2, 1, 2, 1, 1, 0, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1,\n",
       "       1, 1, 2, 1, 1, 1, 1, 0, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 2, 2,\n",
       "       0, 2, 1, 1, 1, 1, 2, 1, 2, 2, 1, 2, 2, 1, 1, 1, 1, 1, 0, 2, 2, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 2, 1, 1,\n",
       "       2, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 2, 0,\n",
       "       1, 1, 1, 0, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1,\n",
       "       2, 2, 1, 1, 2, 2, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1,\n",
       "       1, 2, 1, 1, 1, 2, 2, 1, 2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2,\n",
       "       2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 2, 2, 1,\n",
       "       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1,\n",
       "       1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       0, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LO4w5zOiqDrF"
   },
   "source": [
    "# Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lyv91gJkp-zq",
    "outputId": "0cc159da-8784-4bb8-b59c-ab572580ca62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.04      0.07       129\n",
      "           1       0.48      0.70      0.57       192\n",
      "           2       0.10      0.13      0.12        75\n",
      "\n",
      "    accuracy                           0.38       396\n",
      "   macro avg       0.30      0.29      0.25       396\n",
      "weighted avg       0.35      0.38      0.32       396\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true,y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_lDfOA2EXX0"
   },
   "source": [
    "## Prediction Pipeline\n",
    "Converting indexes back to the Sentiment label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WPHDy-0EqXA1",
    "outputId": "5480b86a-6f03-42eb-fe80-a60d645df0bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input the text:Ich habe die Aufnahmeprüfung beim Landesgymnas.\n",
      "LOW 56.318832\n",
      "AVERAGE 60.72657\n",
      "HIGH 56.320847\n"
     ]
    }
   ],
   "source": [
    "texts = input(str('input the text:'))\n",
    "x_val = tokenizer(\n",
    "    text=texts,\n",
    "    add_special_tokens=True,\n",
    "    max_length=256,\n",
    "    truncation=True,\n",
    "    padding='max_length', \n",
    "    return_tensors='tf',\n",
    "    return_token_type_ids = False,\n",
    "    return_attention_mask = True,\n",
    "    verbose = True) \n",
    "validation = sentiment_model.predict({'input_ids':x_val['input_ids'],'attention_mask':x_val['attention_mask']})*100\n",
    "for key , value in zip(encoded_dict.keys(),validation[0]):\n",
    "    print(key,value)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Personality_BERT_Huggingface.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
